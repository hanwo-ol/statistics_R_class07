이번 예제는 k-평균 군집분석의 중요한 특징인 **"초기치 민감성(Sensitivity to initial values)"**에 대해 다루고 있습니다. 
그림은 뺐습니다다

### 분석의 목표: k-평균 알고리즘의 작동 방식 이해하기

k-평균 알고리즘은 다음과 같은 순서로 작동합니다.
1.  **초기 중심점 선택**: k개의 점을 임의로(randomly) 선택하여 초기 군집 중심으로 삼는다.
2.  **군집 할당**: 모든 데이터 점을 가장 가까운 중심점에 할당하여 k개의 군집을 만든다.
3.  **중심점 업데이트**: 각 군집에 속한 점들의 평균을 계산하여 새로운 중심점으로 업데이트한다.
4.  **반복**: 2번과 3번 과정을 군집의 변화가 더 이상 없을 때까지 반복한다.

여기서 핵심은 **1번 단계**입니다. **어떤 점을 초기 중심점으로 선택하느냐에 따라 최종 군집 결과가 달라질 수 있습니다.** 이번 분석의 목표는 이 현상을 직접 확인하고, k-평균 알고리즘의 이러한 특징을 이해하는 것입니다.

---

### 단계별 코드 및 결과 상세 해설

#### 1. 데이터 준비 및 1차 군집분석

```R
> # 초기치 선택 예제
> set.seed(5)
> xdata=iris[, c(2, 4)]
> str(xdata)
> summary(xdata)
> # 1차 군집분석
> km1 <- kmeans(xdata, 3)
> km1
```

##### **함수 기능 설명**

*   `set.seed(5)`: 결과 재현을 위해 난수 생성 시드(seed)를 5로 고정합니다. `kmeans` 함수는 내부적으로 초기 중심점을 '랜덤'하게 선택하기 때문에, 이 코드가 없으면 실행할 때마다 결과가 달라질 수 있습니다.
*   `xdata=iris[, c(2, 4)]`: R의 내장 데이터셋인 `iris`(붓꽃) 데이터에서 2번째 열(`Sepal.Width`, 꽃받침 너비)과 4번째 열(`Petal.Width`, 꽃잎 너비)만 선택하여 `xdata`로 저장합니다.
*   `str(xdata)`, `summary(xdata)`: 데이터의 구조와 기초 통계량을 확인합니다. 150개의 관측치와 2개의 변수로 구성된 데이터프레임입니다.
*   `km1 <- kmeans(xdata, 3)`: 붓꽃 데이터는 3개의 품종(setosa, versicolor, virginica)으로 구성되어 있으므로, 군집의 개수(k)를 **3**으로 설정하여 1차 군집분석을 수행하고 결과를 `km1`에 저장합니다.

##### **결과(`km1` 출력) 상세 해석**
```
K-means clustering with 3 clusters of sizes 49, 53, 48
```
*   **해석**: 150개의 데이터를 각각 49개, 53개, 48개를 포함하는 3개의 군집으로 나누었습니다.

```
Cluster means:
  Sepal.Width Petal.Width
1    3.451020    0.244898
2    2.707547    1.309434
3    3.041667    2.052083
```
*   **해석**: 3개 군집의 중심 좌표입니다.
    *   **군집 1**: 꽃받침은 넓고(`Sepal.Width` 큼), 꽃잎은 매우 좁은(`Petal.Width` 작음) 특징을 가집니다. (이는 setosa 품종의 특징과 일치합니다.)
    *   **군집 2**: 꽃받침은 좁고(`Sepal.Width` 작음), 꽃잎은 중간 너비의 특징을 가집니다. (versicolor 품종의 특징)
    *   **군집 3**: 꽃받침은 중간 너비이고, 꽃잎은 넓은(`Petal.Width` 큼) 특징을 가집니다. (virginica 품종의 특징)

```
(between_SS / total_SS =  82.1 %)
```
*   **해석**: 군집 간 차이가 전체 데이터 변동의 82.1%를 설명합니다. 이는 군집들이 매우 잘 분리되었음을 의미합니다.

---

#### 2. 2차 군집분석 (다른 초기치로 시작)

```R
> # 2차 군집분석 (다른 초기치로 시작)
> km2 <- kmeans(xdata, 3)
> km2
```

##### **함수 기능 설명**

*   `km2 <- kmeans(xdata, 3)`: **`set.seed()`를 다시 실행하지 않고** `kmeans` 함수를 그대로 한 번 더 실행합니다. `set.seed()`가 없으면 `kmeans`는 **이전과는 다른 새로운 랜덤 초기 중심점**을 선택하여 분석을 시작하게 됩니다.

##### **결과(`km2` 출력) 상세 해석**
```
K-means clustering with 3 clusters of sizes 49, 48, 53

Cluster means:
  Sepal.Width Petal.Width
1    3.451020    0.244898
2    3.041667    2.052083
3    2.707547    1.309434
```
*   **결과 비교**:
    *   **`km1`의 군집 2 중심**: (2.707, 1.309)
    *   **`km1`의 군집 3 중심**: (3.041, 2.052)
    *   **`km2`의 군집 2 중심**: (3.041, 2.052)
    *   **`km2`의 군집 3 중심**: (2.707, 1.309)
*   **해석**:
    *   `km2`의 결과는 `km1`의 결과와 **군집의 중심 좌표, 군집의 크기, (between_SS / total_SS) 비율 등 모든 핵심 지표가 완전히 동일합니다.**
    *   **단 하나 달라진 점은 '군집 번호(label)'입니다.** 1차 분석에서는 (2.707, 1.309) 중심을 가진 군집이 2번이었는데, 2차 분석에서는 3번으로 불리게 되었습니다.
    *   이는 **초기 중심점의 선택이 달라졌지만, 알고리즘이 반복을 거쳐 결국 동일한 최종 군집 구조(최적해, optimal solution)에 도달했음**을 의미합니다. 군집의 이름표(번호)만 바뀐 것일 뿐, 군집의 내용물과 특성은 완전히 같습니다.

### 왜 `nstart` 옵션이 중요한가?

이번 예제에서는 운 좋게도 다른 초기치에서 시작했음에도 불구하고 동일한 최적해를 찾았습니다. 하지만 데이터 구조가 복잡한 경우, **초기 중심점의 선택에 따라 완전히 다른 군집 결과(지역 최적해, local optimum)에 빠져버릴 수 있습니다.** 즉, 더 좋지 않은 군집 결과가 나올 수 있다는 의미입니다.

이를 방지하기 위해 `kmeans` 함수는 **`nstart`**라는 매우 중요한 옵션을 제공합니다.

```R
# 올바른 사용법 예시
kmeans(xdata, 3, nstart = 25)
```
*   **`nstart = 25`의 의미**:
    1.  서로 다른 랜덤 초기 중심점 세트를 **25번** 생성하여 k-평균 군집분석을 독립적으로 25번 실행합니다.
    2.  25번의 실행 결과 중에서 **군집 내 응집도(WSS)가 가장 낮은 (가장 잘 뭉쳐진) 결과를 최종 결과로 선택하여 반환**합니다.

*   **효과**: 이 옵션을 사용하면, 단 한 번의 실행으로 운 나쁘게 지역 최적해에 빠질 위험을 크게 줄이고, **전역 최적해(global optimum)에 가까운 안정적인 군집 결과를 얻을 확률을 매우 높여줍니다.**

### 종합 결론

1.  **k-평균의 초기치 민감성**: k-평균 알고리즘은 **임의로 선택되는 초기 중심점에 따라 최종 결과가 달라질 수 있는** 특징을 가집니다.
2.  **최적해와 지역 최적해**: 운이 좋으면 다른 초기치에서도 동일한 최적해를 찾을 수 있지만(이번 예제처럼), 운이 나쁘면 더 좋지 않은 결과인 지역 최적해에 머무를 수 있습니다.
3.  **해결책 (`nstart` 옵션)**: 실제 데이터 분석에서는 **반드시 `nstart` 옵션을 충분히 큰 값(예: 25 또는 50)으로 설정**하여 여러 번의 시도 중 가장 좋은 결과를 선택하도록 해야 합니다. 이것이 k-평균 군집분석에서 안정적이고 신뢰성 있는 결과를 얻기 위한 표준적인 방법입니다.





---

이번 예제는 `nstart` 옵션의 실제 효과를 확인하고, 이어서 최적의 군집 개수(k)를 찾는 대표적인 방법인 **엘보우 방법(Elbow Method)**에 대해 다루고 있습니다. 

### 분석의 흐름: 안정적인 결과 얻기 및 최적의 k 찾기

1.  **`nstart` 옵션의 효과 검증**: `nstart=1`(기본값)과 `nstart=25`를 사용했을 때 결과가 어떻게 달라지는지(또는 같은지) 확인하여 `nstart` 사용의 중요성을 다시 한번 이해합니다.
2.  **엘보우 방법(Elbow Method) 적용**: 군집의 개수 k를 1부터 10까지 변화시키면서 각 k값에 대한 군집화 성능 지표를 계산하고, 이를 그래프로 그려 "팔꿈치(elbow)"처럼 꺾이는 지점을 찾아 최적의 k를 결정합니다.

---

### 단계별 코드 및 결과 상세 해설

#### 1. `nstart` 옵션의 효과 검증

```R
> # nstart=1 (기본값)
> km.res <- kmeans(xdata, 3)
> km.res$betweenss/km.res$totss # information with 1 start
[1] 0.8206567

> # nstart=25 (25회 반복 후 최적 결과 저장)
> km.res25 <- kmeans(xdata, 3, nstart = 25)
> km.res25$betweenss/km.res25$totss # information with 25 starts
[1] 0.8206567
```

##### **함수 기능 및 결과 해석**

*   `km.res <- kmeans(xdata, 3)`: `nstart` 옵션을 지정하지 않으면 기본값인 `nstart=1`이 적용됩니다. 즉, 랜덤 초기 중심점 세트 **단 하나**만으로 군집분석을 1회 수행합니다.
*   `km.res25 <- kmeans(xdata, 3, nstart = 25)`: 서로 다른 랜덤 초기 중심점 세트로 군집분석을 **25회** 수행하고, 그중 가장 좋은 결과를 `km.res25`에 저장합니다.
*   `$betweenss / $totss`: 군집 간 분산이 전체 분산에서 차지하는 비율로, **군집이 얼마나 잘 분리되었는지**를 나타내는 성능 지표입니다. 0과 1 사이의 값을 가지며, 1에 가까울수록 좋습니다.
*   **결과 비교**: `nstart=1`일 때와 `nstart=25`일 때의 성능 지표(`betweenss/totss`)가 **0.8206567**로 동일하게 나왔습니다.
*   **결론**: 이 `iris` 데이터의 경우, 데이터의 군집 구조가 매우 뚜렷하기 때문에, 한 번의 시도(`nstart=1`)만으로도 운 좋게 최적의 군집 결과를 찾을 수 있었습니다. 하지만 이는 항상 보장되는 것이 아니므로, **실제 분석에서는 반드시 `nstart`를 충분히 큰 값으로 설정하여 안정적인 결과를 확보하는 것이 표준적인 분석 절차입니다.**

#### 2. 군집화 결과 확인 및 시각화

```R
> km.res25$cluster # 분류된 군집 번호
> km.res25$centers # 각 군집의 중심
> km.res25$size    # 군집별 관측치 개수
> # 군집화 결과 시각화
> plot(xdata, pch=20, col=km.res25$cluster+1,
+      cex=1.5, main="k-means clustering with 3 clusters")
```
##### **함수 기능 및 결과 해석**
*   `km.res25$cluster`, `$centers`, `$size`: `nstart=25`로 얻은 안정적인 군집화 결과의 세부 내용을 확인합니다. 각 데이터가 어떤 군집에 속하는지, 각 군집의 특성(중심 좌표)은 무엇인지, 각 군집의 크기는 얼마인지를 보여줍니다.
*   `plot(...)`: 군집분석 결과를 시각화합니다.
    *   `col=km.res25$cluster+1`: `km.res25$cluster`는 군집 번호(1, 2, 3)를 담고 있습니다. 여기에 1을 더해 R의 기본 색상 팔레트에서 2번(빨강), 3번(초록), 4번(파랑) 색상을 사용하도록 지정합니다.
    *   **그래프 해석**: 시각화된 결과를 보면, 3개의 군집이 비교적 명확하게 분리되어 있음을 한눈에 확인할 수 있습니다. `k=3`이 이 데이터에 꽤 적합한 군집 개수임을 짐작할 수 있습니다.

---

#### 3. 엘보우 방법(Elbow Method)으로 최적의 k 찾기

```R
> ##### Elbow Method로 최적 군집 수 찾기(page30)######
> N = 10
> infom = c()
> # 군집의 개수를 1~10까지 변화시키며 수행
> for (i in 1:N)
+ {
+   km.out = kmeans(xdata, i, nstart=10)
+   infom[i] = km.out$betweenss/km.out$totss
+ }
> infom
> plot(1:N, infom, type="b", ...)
```

##### **함수 기능 및 코드 흐름 설명**

1.  `N = 10`: 최대 몇 개의 군집까지 테스트해볼지 설정합니다.
2.  `infom = c()`: 각 k값에 대한 성능 지표를 저장할 빈 벡터를 생성합니다.
3.  `for (i in 1:N)`: `for` 반복문을 이용해 `i`에 1부터 10까지 차례대로 할당하며 아래 코드를 반복합니다.
4.  `km.out = kmeans(xdata, i, nstart=10)`: 군집의 개수를 `i`개로 설정하여 k-평균 군집분석을 수행합니다. (여기서는 `nstart=10`으로 설정하여 안정성을 높였습니다.)
5.  `infom[i] = km.out$betweenss/km.out$totss`: 계산된 성능 지표(`betweenss/totss`)를 `infom` 벡터의 `i`번째 위치에 저장합니다.
6.  반복이 끝나면 `infom` 벡터에는 k=1일 때부터 k=10일 때까지의 성능 지표가 순서대로 저장됩니다.
7.  `plot(1:N, infom, type="b", ...)`: x축은 군집의 개수(k), y축은 성능 지표(`infom`)로 하는 꺾은선 그래프를 그립니다. `type="b"`는 점(point)과 선(line)을 모두 그리라는 의미입니다.

##### **결과(`plot` 그래프) 상세 해석**

*   **그래프의 의미**: 이 그래프는 **군집의 개수(k)가 늘어남에 따라 군집의 분리도(`betweenss/totss`)가 어떻게 향상되는지**를 보여줍니다.
*   **x축**: 군집의 개수 (Number of clusters, k)
*   **y축**: 군집화 성능 지표 (Information, `betweenss/totss`)

*   **엘보우(Elbow) 지점 찾기**:
    *   **k=1에서 k=2로 갈 때**: 성능 지표가 0에서 0.68로 **급격하게 증가**합니다.
    *   **k=2에서 k=3로 갈 때**: 성능 지표가 0.68에서 0.82로 **여전히 큰 폭으로 증가**합니다.
    *   **k=3에서 k=4로 갈 때**: 성능 지표가 0.82에서 0.86으로 증가하긴 하지만, **증가폭이 눈에 띄게 둔화**됩니다.
    *   **k=4 이후**: k가 늘어나도 성능 지표는 아주 조금씩만 증가하며 거의 수평을 이룹니다.

*   **"팔꿈치"는 어디인가?**: 그래프의 기울기가 급격히 꺾여 완만해지기 시작하는 지점, 즉 **사람의 팔꿈치(Elbow)처럼 보이는 지점**은 바로 **k=3** 입니다.
*   **엘보우 방법의 원리**: k를 1씩 늘릴 때마다 얻는 성능 향상(정보량 증가)의 이득과, 모델이 불필요하게 복잡해지는 비용을 비교하는 것입니다. "팔꿈치" 지점은 **비용 대비 효율이 가장 좋은 지점**, 즉 그 이상으로 군집을 나누는 것은 큰 의미가 없는 지점을 의미합니다.

### 종합 결론

1.  **`nstart`의 중요성**: 안정적인 군집분석 결과를 얻기 위해 `nstart` 옵션을 사용하는 것은 매우 중요하며, 이는 k-평균 군집분석의 표준적인 절차입니다.
2.  **최적의 k 찾기**: `iris` 데이터에 대해 엘보우 방법을 적용한 결과, **k=3** 지점에서 그래프의 기울기가 급격히 꺾이는 "팔꿈치" 현상이 관찰되었습니다.
3.  **최종 결론**: 따라서, 이 데이터에 가장 적합한 군집의 개수는 **3개**라고 판단할 수 있습니다. 이는 우리가 이미 알고 있는 붓꽃의 실제 품종 개수(3개)와 일치하는 결과로, 엘보우 방법이 최적의 k를 찾는 데 매우 효과적인 방법임을 보여줍니다.


